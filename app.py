import os
os.environ["PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION"] = "python"

import streamlit as st
from bs4 import BeautifulSoup
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch

st.set_page_config(page_title="è£½è–¬ã‚·ãƒ³ãƒãƒ»AIè¦ç´„ãƒ„ãƒ¼ãƒ«", page_icon="ğŸ’Š")

@st.cache_resource
def load_summary_model():
    model_name = "google/mt5-small"
    # pipelineã‚’ä½¿ã‚ãšã€ç›´æ¥ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã‚€
    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name, low_cpu_mem_usage=True)
    return tokenizer, model

st.title("ğŸ’Š è£½è–¬ã‚·ãƒ³ãƒã‚¸ã‚¦ãƒ AIè¦ç´„ãƒ„ãƒ¼ãƒ«")

with st.spinner("AIæº–å‚™ä¸­..."):
    tokenizer, model = load_summary_model()

raw_html = st.text_area("SharePointã®HTMLã‚’è²¼ã‚Šä»˜ã‘ã¦ãã ã•ã„", height=200)

if st.button("AIè¦ç´„ã‚’å®Ÿè¡Œ"):
    if raw_html:
        soup = BeautifulSoup(raw_html, "html.parser")
        cleaned_text = soup.get_text(separator="\n", strip=True)
        input_text = "summarize: " + cleaned_text[:800] # ãƒ¢ãƒ‡ãƒ«ã¸ã®æ˜ç¤ºçš„ãªæŒ‡ç¤º
        
        st.subheader("ğŸ“ AIè¦ç´„çµæœ")
        with st.spinner("è¦ç´„ã‚’ç”Ÿæˆä¸­..."):
            try:
                # pipelineã‚’ä½¿ã‚ãšã«ç›´æ¥è¨ˆç®—ã™ã‚‹æ‰‹é †
                inputs = tokenizer.encode(input_text, return_tensors="pt", max_length=800, truncation=True)
                outputs = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)
                summary_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
                
                st.success(summary_text)
            except Exception as e:
                st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        
        with st.expander("å…¨æ–‡è¡¨ç¤º"):
            st.write(cleaned_text)